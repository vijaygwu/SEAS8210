{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLd9ssAAhqYTN3OppWxUnv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijaygwu/SEAS8210/blob/main/Embedings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IDs → embeddings lookup, cosine similarity, and sinusoidal positions.\n",
        "# No external packages required besides numpy.\n",
        "\n",
        "import re\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(13)  # deterministic demo\n",
        "\n",
        "def normalize(text):\n",
        "    # Minimal normalization: collapse whitespace and ensure basic ASCII apostrophe normalization\n",
        "    text = text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "    text = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    return text\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    # Simple heuristic tokenizer splitting punctuation\n",
        "    text = normalize(text)\n",
        "    # separate punctuation\n",
        "    text = re.sub(r\"([.,!?;:()\\[\\]{}])\", r\" \\1 \", text)\n",
        "    # split contractions: can't -> can n't\n",
        "    text = re.sub(r\"(\\w)('t|'ll|'re|'ve|'s|'d|'m)\\b\", r\"\\1 \\2\", text)\n",
        "    toks = [t for t in text.split(\" \") if t]\n",
        "    return toks\n",
        "\n",
        "def char_tokenize(text):\n",
        "    text = normalize(text)\n",
        "    # Prefix each word with a \"▁\" marker like SentencePiece\n",
        "    words = text.split(\" \")\n",
        "    chars = []\n",
        "    for w in words:\n",
        "        if not w: continue\n",
        "        chars.append(\"▁\")\n",
        "        chars.extend(list(w))\n",
        "    return chars\n",
        "\n",
        "# ---- Tiny BPE trainer/encoder (character level → subword merges) ----\n",
        "\n",
        "def get_vocab_from_corpus(texts):\n",
        "    \"\"\"Return list of words split as character sequences with a '▁' prefix per word.\"\"\"\n",
        "    vocab = []\n",
        "    for t in texts:\n",
        "        t = normalize(t)\n",
        "        for w in t.split(\" \"):\n",
        "            if not w:\n",
        "                continue\n",
        "            vocab.append([\"▁\"] + list(w))\n",
        "    return vocab\n",
        "\n",
        "def count_pair_stats(vocab):\n",
        "    \"\"\"Count adjacent pair frequencies across all tokens in vocab.\"\"\"\n",
        "    pairs = Counter()\n",
        "    for token_list in vocab:\n",
        "        for i in range(len(token_list)-1):\n",
        "            pairs[(token_list[i], token_list[i+1])] += 1\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(vocab, pair_to_merge):\n",
        "    \"\"\"Merge chosen pair in all token lists.\"\"\"\n",
        "    a, b = pair_to_merge\n",
        "    new_vocab = []\n",
        "    for token_list in vocab:\n",
        "        i = 0\n",
        "        merged = []\n",
        "        while i < len(token_list):\n",
        "            if i < len(token_list)-1 and token_list[i] == a and token_list[i+1] == b:\n",
        "                merged.append(a+b)\n",
        "                i += 2\n",
        "            else:\n",
        "                merged.append(token_list[i])\n",
        "                i += 1\n",
        "        new_vocab.append(merged)\n",
        "    return new_vocab\n",
        "\n",
        "def train_bpe(texts, num_merges=50):\n",
        "    vocab = get_vocab_from_corpus(texts)\n",
        "    merges = []\n",
        "    for _ in range(num_merges):\n",
        "        stats = count_pair_stats(vocab)\n",
        "        if not stats:\n",
        "            break\n",
        "        pair, freq = stats.most_common(1)[0]\n",
        "        # Stop if merging doesn't change anything meaningful\n",
        "        if freq < 2:\n",
        "            break\n",
        "        merges.append(pair)\n",
        "        vocab = merge_vocab(vocab, pair)\n",
        "    # Build a ranking for merges (earlier is higher priority)\n",
        "    merge_ranks = {pair: i for i, pair in enumerate(merges)}\n",
        "    return merges, merge_ranks\n",
        "\n",
        "def bpe_encode_word(word, merge_ranks):\n",
        "    \"\"\"Encode a single word by greedily applying highest-priority merges present.\"\"\"\n",
        "    # Start from char-level with a prefixed whitespace marker\n",
        "    tokens = [\"▁\"] + list(word)\n",
        "    # Keep merging while any adjacent pair is in merge_ranks\n",
        "    while True:\n",
        "        candidates = []\n",
        "        for i in range(len(tokens)-1):\n",
        "            pair = (tokens[i], tokens[i+1])\n",
        "            if pair in merge_ranks:\n",
        "                candidates.append((merge_ranks[pair], i, pair))\n",
        "        if not candidates:\n",
        "            break\n",
        "        # Highest priority = smallest rank\n",
        "        _, i, pair = min(candidates, key=lambda x: x[0])\n",
        "        a, b = pair\n",
        "        tokens = tokens[:i] + [a+b] + tokens[i+2:]\n",
        "    return tokens\n",
        "\n",
        "def bpe_tokenize(text, merge_ranks):\n",
        "    text = normalize(text)\n",
        "    out = []\n",
        "    for w in text.split(\" \"):\n",
        "        if not w: continue\n",
        "        out.extend(bpe_encode_word(w, merge_ranks))\n",
        "    return out\n",
        "\n",
        "# ---- Demo content ----\n",
        "\n",
        "train_texts = [\n",
        "    \"low lower lowest\",\n",
        "    \"new newer newest\",\n",
        "    \"I can't believe it's not butter\",\n",
        "    \"unbelievable unbeliever believable believing\",\n",
        "    \"Tokenizer tokenization tokens tokenized tokenizing\",\n",
        "    \"cats catlike dogs doglike running runner runs\"\n",
        "]\n",
        "\n",
        "print(\"=== Training a tiny BPE on a small classroom corpus ===\")\n",
        "merges, merge_ranks = train_bpe(train_texts, num_merges=80)\n",
        "print(f\"Learned {len(merges)} merges. First 15 merges:\")\n",
        "for i, p in enumerate(merges[:15], start=1):\n",
        "    print(f\"{i:>2}: {p[0]} + {p[1]} → {p[0]+p[1]}\")\n",
        "\n",
        "sample = \"I can't believe it's not butter and unbelievable newness\"\n",
        "print(\"\\n=== Sample sentence ===\")\n",
        "print(sample)\n",
        "\n",
        "print(\"\\nWhitespace tokens:\")\n",
        "print(whitespace_tokenize(sample))\n",
        "\n",
        "print(\"\\nCharacter tokens with '▁' markers:\")\n",
        "print(char_tokenize(sample))\n",
        "\n",
        "print(\"\\nTiny BPE tokens (learned from corpus):\")\n",
        "bpe_toks = bpe_tokenize(sample, merge_ranks)\n",
        "print(bpe_toks)\n",
        "\n",
        "# ---- Map tokens to IDs (build a toy vocab from BPE on training texts) ----\n",
        "print(\"\\n=== Building a toy vocabulary from BPE over training texts ===\")\n",
        "\n",
        "vocab_set = set()\n",
        "for t in train_texts:\n",
        "    for tok in bpe_tokenize(t, merge_ranks):\n",
        "        vocab_set.add(tok)\n",
        "\n",
        "# Add specials\n",
        "specials = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n",
        "vocab = specials + sorted(vocab_set)\n",
        "token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
        "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
        "print(f\"Vocab size: {len(vocab)}\")\n",
        "\n",
        "# Encode the sample to IDs\n",
        "def to_ids(tokens):\n",
        "    return [token_to_id.get(t, token_to_id[\"<UNK>\"]) for t in tokens]\n",
        "\n",
        "ids = to_ids(bpe_toks)\n",
        "print(\"IDs:\", ids)\n",
        "\n",
        "# ---- Embeddings: lookup & cosine similarity ----\n",
        "d_model = 16\n",
        "E = (np.random.randn(len(vocab), d_model) / np.sqrt(d_model)).astype(np.float32)\n",
        "\n",
        "def embed(ids):\n",
        "    return E[ids]  # [seq_len, d_model]\n",
        "\n",
        "def cosine(a, b):\n",
        "    a = a / (np.linalg.norm(a) + 1e-9)\n",
        "    b = b / (np.linalg.norm(b) + 1e-9)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "print(\"\\n=== Embedding lookup ===\")\n",
        "print(f\"E.shape = {E.shape}  # [vocab_size, d_model]\")\n",
        "\n",
        "# Pick a few tokens to compare\n",
        "probe_tokens = [\"▁new\", \"▁low\", \"er\", \"est\", \"un\", \"able\"]\n",
        "probe_ids = [token_to_id.get(t, token_to_id[\"<UNK>\"]) for t in probe_tokens]\n",
        "for t, i in zip(probe_tokens, probe_ids):\n",
        "    vec = E[i]\n",
        "    print(f\"Token {t!r} → ID {i} → first 4 dims {vec[:4].round(3)}\")\n",
        "\n",
        "print(\"\\nCosine similarities among probes (random init; for illustration):\")\n",
        "for i, ti in enumerate(probe_tokens):\n",
        "    for j in range(i+1, len(probe_tokens)):\n",
        "        tj = probe_tokens[j]\n",
        "        sim = cosine(E[token_to_id.get(ti, token_to_id['<UNK>'])],\n",
        "                     E[token_to_id.get(tj, token_to_id['<UNK>'])])\n",
        "        print(f\"cos({ti:>5}, {tj:>5}) = {sim:+.3f}\")\n",
        "\n",
        "# ---- Positional encodings (sin/cos) ----\n",
        "def sinusoidal_positions(max_len, d_model):\n",
        "    pe = np.zeros((max_len, d_model), dtype=np.float32)\n",
        "    position = np.arange(0, max_len, dtype=np.float32)[:, None]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2, dtype=np.float32) * (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "print(\"\\n=== Sinusoidal positional encodings (first 3 positions, 8 dims) ===\")\n",
        "pe = sinusoidal_positions(3, 8)\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(pe)\n",
        "\n",
        "print(\"\\n=== Putting it together (toy forward input) ===\")\n",
        "seq_emb = embed(ids[:10])\n",
        "pe10 = sinusoidal_positions(len(seq_emb), d_model)\n",
        "x0 = seq_emb + pe10  # what goes into the first transformer block (toy illustration)\n",
        "print(\"x0 shape:\", x0.shape)\n",
        "print(\"First token embedding+pos (first 4 dims):\", x0[0][:4].round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYx2KLq0mj9e",
        "outputId": "1c636efb-7ef1-4979-8ea3-c8d6d7db259b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training a tiny BPE on a small classroom corpus ===\n",
            "Learned 40 merges. First 15 merges:\n",
            " 1: l + i → li\n",
            " 2: k + e → ke\n",
            " 3: e + r → er\n",
            " 4: b + e → be\n",
            " 5: be + li → beli\n",
            " 6: beli + e → belie\n",
            " 7: belie + v → believ\n",
            " 8: u + n → un\n",
            " 9: o + ke → oke\n",
            "10: oke + n → oken\n",
            "11: ▁ + n → ▁n\n",
            "12: oken + i → okeni\n",
            "13: okeni + z → okeniz\n",
            "14: ▁ + t → ▁t\n",
            "15: ▁ + l → ▁l\n",
            "\n",
            "=== Sample sentence ===\n",
            "I can't believe it's not butter and unbelievable newness\n",
            "\n",
            "Whitespace tokens:\n",
            "['I', 'can', \"'t\", 'believe', 'it', \"'s\", 'not', 'butter', 'and', 'unbelievable', 'newness']\n",
            "\n",
            "Character tokens with '▁' markers:\n",
            "['▁', 'I', '▁', 'c', 'a', 'n', \"'\", 't', '▁', 'b', 'e', 'l', 'i', 'e', 'v', 'e', '▁', 'i', 't', \"'\", 's', '▁', 'n', 'o', 't', '▁', 'b', 'u', 't', 't', 'e', 'r', '▁', 'a', 'n', 'd', '▁', 'u', 'n', 'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'e', '▁', 'n', 'e', 'w', 'n', 'e', 's', 's']\n",
            "\n",
            "Tiny BPE tokens (learned from corpus):\n",
            "['▁', 'I', '▁ca', 'n', \"'\", 't', '▁believ', 'e', '▁', 'i', 't', \"'\", 's', '▁n', 'o', 't', '▁', 'b', 'u', 't', 't', 'er', '▁', 'a', 'n', 'd', '▁unbeliev', 'able', '▁new', 'n', 'es', 's']\n",
            "\n",
            "=== Building a toy vocabulary from BPE over training texts ===\n",
            "Vocab size: 37\n",
            "IDs: [24, 5, 26, 17, 4, 22, 25, 11, 24, 14, 22, 4, 21, 30, 18, 22, 24, 9, 23, 22, 22, 12, 24, 7, 17, 10, 36, 8, 31, 17, 3, 21]\n",
            "\n",
            "=== Embedding lookup ===\n",
            "E.shape = (37, 16)  # [vocab_size, d_model]\n",
            "Token '▁new' → ID 31 → first 4 dims [-0.288 -0.228 -0.263 -0.14 ]\n",
            "Token '▁low' → ID 29 → first 4 dims [ 0.328 -0.314  0.159 -0.067]\n",
            "Token 'er' → ID 12 → first 4 dims [ 0.317  0.064 -0.416 -0.03 ]\n",
            "Token 'est' → ID 13 → first 4 dims [-0.354  0.032 -0.096 -0.388]\n",
            "Token 'un' → ID 3 → first 4 dims [-0.058  0.178  0.113 -0.083]\n",
            "Token 'able' → ID 8 → first 4 dims [ 0.223 -0.385 -0.066 -0.233]\n",
            "\n",
            "Cosine similarities among probes (random init; for illustration):\n",
            "cos( ▁new,  ▁low) = -0.263\n",
            "cos( ▁new,    er) = +0.064\n",
            "cos( ▁new,   est) = +0.534\n",
            "cos( ▁new,    un) = -0.086\n",
            "cos( ▁new,  able) = +0.022\n",
            "cos( ▁low,    er) = -0.189\n",
            "cos( ▁low,   est) = -0.681\n",
            "cos( ▁low,    un) = -0.308\n",
            "cos( ▁low,  able) = +0.410\n",
            "cos(   er,   est) = +0.029\n",
            "cos(   er,    un) = -0.064\n",
            "cos(   er,  able) = +0.163\n",
            "cos(  est,    un) = +0.112\n",
            "cos(  est,  able) = -0.339\n",
            "cos(   un,  able) = -0.202\n",
            "\n",
            "=== Sinusoidal positional encodings (first 3 positions, 8 dims) ===\n",
            "[[ 0.     1.     0.     1.     0.     1.     0.     1.   ]\n",
            " [ 0.841  0.54   0.1    0.995  0.01   1.     0.001  1.   ]\n",
            " [ 0.909 -0.416  0.199  0.98   0.02   1.     0.002  1.   ]]\n",
            "\n",
            "=== Putting it together (toy forward input) ===\n",
            "x0 shape: (10, 16)\n",
            "First token embedding+pos (first 4 dims): [-0.016  1.185 -0.346  1.058]\n"
          ]
        }
      ]
    }
  ]
}